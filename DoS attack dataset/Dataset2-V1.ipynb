{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt;\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn import preprocessing\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "from kneed import DataGenerator, KneeLocator\r\n",
    "from sklearn.neighbors import KNeighborsRegressor\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.calibration import CalibratedClassifierCV\r\n",
    "from sklearn.naive_bayes import BernoulliNB\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn import tree\r\n",
    "from sklearn.ensemble import VotingClassifier\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "##--------------------------------------------------------------------DATA-----------------------------------------------------------##\r\n",
    "\r\n",
    "trainSize = 800000\r\n",
    "testSize =  200000\r\n",
    "size = trainSize+testSize\r\n",
    "data = pd.read_csv(\"mixed2million.csv\", nrows=size)\r\n",
    "data = data.drop(columns=\"No.\")\r\n",
    "columns_titles = [\"Time\", \"Source\", \"Destination\", \"Length\", \"Info\", \"Protocol\"]\r\n",
    "data = data.reindex(columns=columns_titles)\r\n",
    "label_names_training = np.unique(data.iloc[:trainSize,5])\r\n",
    "label_names_testing = np.unique(data.iloc[trainSize:,5])\r\n",
    "\r\n",
    "le = preprocessing.LabelEncoder()\r\n",
    "data.iloc[:,1] = le.fit_transform(data.iloc[:,1])\r\n",
    "data.iloc[:,2] = le.fit_transform(data.iloc[:,2])\r\n",
    "data.iloc[:,4] = le.fit_transform(data.iloc[:,4])\r\n",
    "data.iloc[:,5] = le.fit_transform(data.iloc[:,5])\r\n",
    "\r\n",
    "def chooseObsMan(dic_f, y_train_list):\r\n",
    "    chosenIDs=np.array(())\r\n",
    "    for k,d in dic_f.items():\r\n",
    "        temp=np.where(y_train_list==k)[0]\r\n",
    "        if temp.size < dic_f[k]:\r\n",
    "            temp = dic_f[k]\r\n",
    "        z=np.random.choice(temp,dic_f[k],replace=False)\r\n",
    "        chosenIDs=np.concatenate((chosenIDs,z))\r\n",
    "    return np.int64(chosenIDs)\r\n",
    "\r\n",
    "def label_index(row,threshold=0):\r\n",
    "    \r\n",
    "    if threshold==0:\r\n",
    "        threshold=np.max(row)\r\n",
    "    temp_i=np.where(row>=threshold)[0]\r\n",
    "    if temp_i.shape[0]>0:\r\n",
    "        return temp_i[0]+1\r\n",
    "    else:\r\n",
    "        return -1\r\n",
    "\r\n",
    "y_train_ori = data.iloc[:trainSize,5]\r\n",
    "x_train_ori = data.iloc[:trainSize,:-1]\r\n",
    "#x_train = preprocessing.normalize(data.iloc[:trainSize,:-1])\r\n",
    "\r\n",
    "y_test_ori = data.iloc[trainSize:,5]\r\n",
    "x_test_ori = data.iloc[trainSize:,:-1]\r\n",
    "#x_test = preprocessing.normalize(data.iloc[trainSize:,:-1])\r\n",
    "\r\n",
    "li = {}\r\n",
    "r = int(size/100000 + 1)*30\r\n",
    "for i in np.unique(data.iloc[:,-1]):\r\n",
    "    if data['Protocol'].value_counts()[i] < r:\r\n",
    "        li.update({i : data['Protocol'].value_counts()[i]})    \r\n",
    "    else :\r\n",
    "        li.update({i : r})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.model_selection import KFold\r\n",
    "kfold = KFold(10, True, 1)\r\n",
    "best5 = [4, 5, 7, 16, 25]\r\n",
    "#for train_index, test_index in kfold.split(x_train_ori):\r\n",
    "\t#print('train: %s, test: %s' % (x_train_ori.iloc[train_index].shape, x_train_ori.iloc[test_index].shape))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "Accuracy_list_OurModel = []\r\n",
    "Accuracy_list_OurModel_Testing = []\r\n",
    "Fscore_list_OurModel = []\r\n",
    "Fscore_list_OurModel_Testing = []\r\n",
    "\r\n",
    "#iKn=1\r\n",
    "for train_index, test_index in kfold.split(x_train_ori):\r\n",
    "    #print('KFold :',  iKn)\r\n",
    "    #iKn = iKn + 1\r\n",
    "\r\n",
    "    y_train = y_train_ori.iloc[train_index]\r\n",
    "    x_train = x_train_ori.iloc[train_index]\r\n",
    "\r\n",
    "    y_test = y_train_ori.iloc[test_index]\r\n",
    "    x_test = x_train_ori.iloc[test_index]\r\n",
    "\r\n",
    "    #---------------------------------------------OurModel----------------------------------------\r\n",
    "    clfs ={}\r\n",
    "\r\n",
    "    labelled=np.zeros(x_train.shape[0])\r\n",
    "    labelled[:]=-1\r\n",
    "    labelledIndex=chooseObsMan(li, y_train)\r\n",
    "    labelled[labelledIndex]=y_train.reindex(labelledIndex)\r\n",
    "\r\n",
    "    clf1=BernoulliNB()\r\n",
    "    #clf2=KNeighborsClassifier(n_jobs=-1)\r\n",
    "    clf3=RandomForestClassifier ( n_estimators = 20 , random_state = 0 ,n_jobs=-1)\r\n",
    "    #clf4=SVC(kernel='rbf', probability=True)\r\n",
    "    clf5=tree.DecisionTreeClassifier()\r\n",
    "    clf7=AdaBoostClassifier()\r\n",
    "    #clf8=QuadraticDiscriminantAnalysis()\r\n",
    "\r\n",
    "    #clfs=[('NB', clf1),('KNN', clf2), ('RFC', clf3),('SVC', clf4), ('j48', clf5)]\r\n",
    "    #clfs=[('NB', clf1), ('KNN', clf2), ('RFC', clf3), ('SVC', clf4)]\r\n",
    "    #clfs=[('KNN', clf2), ('RFC', clf3),('SVC', clf4), ('j48', clf5)]\r\n",
    "    #clfs=[('KNN', clf2), ('RFC', clf3), ('j48', clf5)]\r\n",
    "    clfs1=[('NB', clf1),('j48', clf5), ('RFC', clf3)]\r\n",
    "    #clfs=[('RFC', clf3),('SVC', clf4), ('j48', clf5)]\r\n",
    "    #clfs=[('KNN', clf2), ('RFC', clf3),('SVC', clf4)]\r\n",
    "    clfs2=[('NB', clf1)]\r\n",
    "\r\n",
    "    ensemble_models = VotingClassifier(estimators=clfs1, voting='soft')\r\n",
    "\r\n",
    "    threshold=0.5\r\n",
    "    leftNumber=-1\r\n",
    "    temp_left_Group=np.arange(x_train.shape[0])\r\n",
    "    for i in range(0,x_train.shape[0]):\r\n",
    "        temp_left=sum(labelled==-1)\r\n",
    "        #print(i, temp_left)\r\n",
    "\r\n",
    "        #ensemble_models = VotingClassifier(estimators=clfs1, voting='soft')\r\n",
    "        #print(x_train[labelled!=-1].shape)\r\n",
    "        ensemble_models.fit(x_train[labelled!=-1],y_train[labelled!=-1])\r\n",
    "        if temp_left<=0:\r\n",
    "            break\r\n",
    "        predicted=ensemble_models.predict_proba(x_train[labelled==-1])\r\n",
    "        final_decition=np.apply_along_axis(label_index, 1, predicted,threshold)\r\n",
    "        labelled[labelled==-1]=final_decition\r\n",
    "        if temp_left==leftNumber:\r\n",
    "            threshold-0.01\r\n",
    "        threshold_step=0.1\r\n",
    "        leftNumber=temp_left\r\n",
    "        temp_left=sum(labelled==-1)\r\n",
    "        temp_left_Group[i]=temp_left\r\n",
    "        #print(temp_left, threshold, (final_decition != -1).mean() *400000 , (predicted != -1).mean() *400000)\r\n",
    "        if i >0:\r\n",
    "            change_rate=temp_left_Group[i-1] - temp_left_Group[i]\r\n",
    "            #print(change_rate)\r\n",
    "            if(change_rate<1000):\r\n",
    "                threshold=threshold-threshold_step\r\n",
    "\r\n",
    "\r\n",
    "    #print(\"------------------------------------Our Module---------------------------------------\")\r\n",
    "    #print('threshold step=',threshold_step)\r\n",
    "\r\n",
    "    #print('\\n*********Training*********\\n')\r\n",
    "    '''\r\n",
    "    pre=labelled[labelled!=-1]\r\n",
    "    org=y_train[labelled!=-1]\r\n",
    "    pop=metrics.precision_recall_fscore_support(org, pre)\r\n",
    "    '''\r\n",
    "    pre = ensemble_models.predict(x_train)\r\n",
    "    pop = metrics.precision_recall_fscore_support(y_train, pre)\r\n",
    "\r\n",
    "    label_names_training = np.unique(y_train)\r\n",
    "    results = pd.DataFrame()\r\n",
    "    for i in range(0,len(pop)):\r\n",
    "        if len(pop[0]) != np.unique(data.iloc[:,5]).shape[0]:\r\n",
    "            \r\n",
    "            temp = pd.DataFrame([pop[i][:label_names_training.shape[0]]],columns=label_names_training)\r\n",
    "        else :\r\n",
    "            temp = pd.DataFrame([pop[i]],columns=label_names_training)\r\n",
    "        \r\n",
    "        results=results.append(temp, ignore_index=True)\r\n",
    "    #print(len(clfs1),'Classifier')\r\n",
    "\r\n",
    "\r\n",
    "    results.insert(0,'  ',['Precision','Recall','Fscore','Support'])\r\n",
    "    #print(results)\r\n",
    "    resultsDataFrame1 = results\r\n",
    "    correctLabelled=metrics.accuracy_score(y_train, pre,normalize=False)\r\n",
    "    incorrectLabelled=sum(results.iloc[3,1:])-correctLabelled\r\n",
    "    #print([correctLabelled,incorrectLabelled])\r\n",
    "    ourModelTrainingAcc = correctLabelled/(correctLabelled+incorrectLabelled)\r\n",
    "    numbers=results.iloc[3,1:]\r\n",
    "    correctLabelledForEachclass=results.iloc[3,1:] *results.iloc[1,1:]\r\n",
    "    incorrectLabelledForEachclass=results.iloc[3,1:]-correctLabelledForEachclass\r\n",
    "    results1 = pd.DataFrame(columns = ['Total','Correct', 'Incorrect'])\r\n",
    "    results1['Total'] = numbers\r\n",
    "    results1['Correct'] = correctLabelledForEachclass\r\n",
    "    results1['Incorrect'] = incorrectLabelledForEachclass\r\n",
    "    #print(results1)\r\n",
    "\r\n",
    "    #print('\\n*********Testing*********\\n')\r\n",
    "\r\n",
    "    pre = ensemble_models.predict(x_test)\r\n",
    "    pop = metrics.precision_recall_fscore_support(y_test, pre)\r\n",
    "    results = pd.DataFrame()\r\n",
    "\r\n",
    "    #print(len(pop[0]) , label_names_testing.shape)\r\n",
    "\r\n",
    "    for i in range(0,len(pop)):\r\n",
    "        if len(pop[0]) > label_names_testing.shape[0]:\r\n",
    "            temp = pd.DataFrame([pop[i][:label_names_testing.shape[0]]],columns=label_names_testing)\r\n",
    "        else :\r\n",
    "            temp = pd.DataFrame([pop[i]],columns=label_names_testing)\r\n",
    "        \r\n",
    "        results=results.append(temp, ignore_index=True)\r\n",
    "    #print(len(clfs1),'Classifier')\r\n",
    "\r\n",
    "\r\n",
    "    results.insert(0,'  ',['Precision','Recall','Fscore','Support'])\r\n",
    "    #print(results)\r\n",
    "    resultsDataFrame1Test = results\r\n",
    "    correctLabelled=metrics.accuracy_score(y_test, pre,normalize=False)\r\n",
    "    incorrectLabelled=sum(results.iloc[3,1:])-correctLabelled\r\n",
    "    #print([correctLabelled,incorrectLabelled])\r\n",
    "    ourModelTestingAcc = correctLabelled/(correctLabelled+incorrectLabelled)\r\n",
    "    numbers=results.iloc[3,1:]\r\n",
    "    correctLabelledForEachclass=results.iloc[3,1:] *results.iloc[1,1:]\r\n",
    "    incorrectLabelledForEachclass=results.iloc[3,1:]-correctLabelledForEachclass\r\n",
    "    results1Test = pd.DataFrame(columns = ['Total','Correct', 'Incorrect'])\r\n",
    "    results1Test['Total'] = numbers\r\n",
    "    results1Test['Correct'] = correctLabelledForEachclass\r\n",
    "    results1Test['Incorrect'] = incorrectLabelledForEachclass\r\n",
    "    #print(results1Test)\r\n",
    "\r\n",
    "    #Training\r\n",
    "    acc = []\r\n",
    "    for i in range (5) :\r\n",
    "        acc.append(round( ((results1.iloc[i,1] / results1.iloc[i,0])*100) ,2))\r\n",
    "    Accuracy_list_OurModel.append(acc)\r\n",
    "\r\n",
    "    Facc = []\r\n",
    "    for i in range (5) :\r\n",
    "        Facc.append(round( (resultsDataFrame1.iloc[2,i+1])*100 ,2))\r\n",
    "    Fscore_list_OurModel.append(Facc)\r\n",
    "\r\n",
    "    #Testing\r\n",
    "    acc = []\r\n",
    "    for i in range (5) :\r\n",
    "        acc.append(round( ((results1Test.iloc[i,1] / results1Test.iloc[i,0])*100) ,2))\r\n",
    "    Accuracy_list_OurModel_Testing.append(acc)\r\n",
    "\r\n",
    "    Facc = []\r\n",
    "    for i in range (5) :\r\n",
    "        Facc.append(round( (resultsDataFrame1Test.iloc[2,i+1])*100 ,2))\r\n",
    "    Fscore_list_OurModel_Testing.append(Facc)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "Accuracy_list_Paper1 = []\r\n",
    "Accuracy_list_Paper1_Tesing = []\r\n",
    "Fscore_list_Paper1 = []\r\n",
    "Fscore_list_Paper1_Tesing = []\r\n",
    "\r\n",
    "iKn=1\r\n",
    "for train_index, test_index in kfold.split(x_train_ori):\r\n",
    "    print('KFold :',  iKn)\r\n",
    "    iKn = iKn + 1\r\n",
    "\r\n",
    "    y_train = y_train_ori.iloc[train_index]\r\n",
    "    x_train = x_train_ori.iloc[train_index]\r\n",
    "\r\n",
    "    y_test = y_train_ori.iloc[test_index]\r\n",
    "    x_test = x_train_ori.iloc[test_index]\r\n",
    "\r\n",
    "    ##--------------------------------------------------------------------Paper1-----------------------------------------------------------##\r\n",
    "\r\n",
    "\r\n",
    "    #method to find the nearest clusters :--- output is 5\r\n",
    "    ## ---->>> takes so  much time use it if u want\r\n",
    "    def findBestclusters():\r\n",
    "        kmeans_kwargs = { \"init\": \"random\", \"n_init\": 10, \"max_iter\": 300, \"random_state\": 42, }\r\n",
    "        # A list holds the SSE values for each k\r\n",
    "        max_clusters = 10\r\n",
    "        sse = []\r\n",
    "        for k in range(1, max_clusters):\r\n",
    "            kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\r\n",
    "            kmeans.fit(x_train)\r\n",
    "            sse.append(kmeans.inertia_)\r\n",
    "\r\n",
    "        plt.style.use(\"fivethirtyeight\")\r\n",
    "        plt.plot(range(1, max_clusters), sse)\r\n",
    "        plt.xticks(range(1, max_clusters))\r\n",
    "        plt.xlabel(\"Number of Clusters\")\r\n",
    "        plt.ylabel(\"SSE\")\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "        kl = KneeLocator(range(1, max_clusters), sse, curve=\"convex\", direction=\"decreasing\")\r\n",
    "        return kl.elbow\r\n",
    "\r\n",
    "    #result is 2\r\n",
    "\r\n",
    "    #print(findBestclusters())      #result is 2\r\n",
    "\r\n",
    "    number_clusters = np.unique(y_train)[-1]\r\n",
    "    #print(number_clusters)\r\n",
    "    kmeans = KMeans( init=\"random\", n_clusters=number_clusters, n_init=10, max_iter=300, random_state=42).fit(x_train)\r\n",
    "\r\n",
    "    # Final locations of the centroid\r\n",
    "    #print(kmeans.cluster_centers_)\r\n",
    "\r\n",
    "    # The number of iterations required to converge\r\n",
    "    #print(kmeans.n_iter_)\r\n",
    "    #print( kmeans.labels_[:])\r\n",
    "\r\n",
    "    clusters = np.full((number_clusters, number_clusters+1), 0, dtype=int)\r\n",
    "    #print(clusters.shape)\r\n",
    "    for i in range(kmeans.labels_.shape[0]):\r\n",
    "        #print( kmeans.labels_[i], y_train[i])\r\n",
    "        clusters[ kmeans.labels_[i] ][ y_train.iloc[i] ] = clusters[ kmeans.labels_[i] ][ y_train.iloc[i] ] + 1\r\n",
    "        \r\n",
    "    arr = np.arange(0, number_clusters, 1).tolist()\r\n",
    "\r\n",
    "    df = pd.DataFrame(clusters[:,1:number_clusters+1]\r\n",
    "                        , index = arr\r\n",
    "                        , columns = arr)\r\n",
    "\r\n",
    "\r\n",
    "    #print(\"------------------------------------Paper 1---------------------------------------\")\r\n",
    "\r\n",
    "    #print('total number of points in each cluster: \\n')\r\n",
    "    #print(df)\r\n",
    "    #print(clusters)\r\n",
    "    #print('\\nLABEL OF EACH CLUSTER :\\ncluster0-->Probe\\ncluster1-->DoS\\ncluster2-->normal\\ncluster3-->R2L\\ncluster4-->Probe\\ncluster5-->DoS\\ncluster6-->U2R')\r\n",
    "    #print('cluster7-->Probe\\ncluster8-->normal\\ncluster9-->normal')\r\n",
    "\r\n",
    "    #KNN model using Kmeans results\r\n",
    "    clustering_out =[]\r\n",
    "    for i in range(clusters.shape[0]):\r\n",
    "        clustering_out.append( np.argmax(np.array(clusters[i])) )\r\n",
    "\r\n",
    "    knn_model = KNeighborsRegressor(n_neighbors=1)\r\n",
    "    knn_model.fit(kmeans.cluster_centers_, clustering_out)\r\n",
    "\r\n",
    "    #print('\\n*********Training*********\\n')\r\n",
    "\r\n",
    "    train_preds = knn_model.predict(x_train)\r\n",
    "\r\n",
    "    #print((train_preds == y_train).mean())\r\n",
    "    Paper1TrainingAcc = (train_preds == y_train).mean()\r\n",
    "\r\n",
    "    label_names_training = np.unique(y_train)\r\n",
    "    matrices = metrics.precision_recall_fscore_support(y_train, train_preds)\r\n",
    "    resultsDataFrame = pd.DataFrame()\r\n",
    "    for i in range(0,len(matrices)):\r\n",
    "        if len(matrices[0]) > label_names_training.shape[0]:\r\n",
    "            temp = pd.DataFrame([matrices[i][:label_names_training.shape[0]]],columns=label_names_training)\r\n",
    "        else :\r\n",
    "            temp = pd.DataFrame([matrices[i]],columns=label_names_training)\r\n",
    "        #temp = pd.DataFrame([matrices[i]],columns=label_names_training)\r\n",
    "        resultsDataFrame = resultsDataFrame.append(temp, ignore_index=True)\r\n",
    "    resultsDataFrame.insert(0,'  ',['Precision','Recall','Fscore','Support'])\r\n",
    "\r\n",
    "    #print(resultsDataFrame)\r\n",
    "    resultsDataFrame2 = resultsDataFrame\r\n",
    "\r\n",
    "    correctLabelled=metrics.accuracy_score(y_train, train_preds,normalize=False)\r\n",
    "    incorrectLabelled=sum(resultsDataFrame.iloc[3,1:]) - correctLabelled\r\n",
    "    #print([correctLabelled,incorrectLabelled])\r\n",
    "\r\n",
    "    numbers=resultsDataFrame.iloc[3,1:]\r\n",
    "    correctLabelledForEachclass=resultsDataFrame.iloc[3,1:] *resultsDataFrame.iloc[1,1:]\r\n",
    "    incorrectLabelledForEachclass=resultsDataFrame.iloc[3,1:]-correctLabelledForEachclass\r\n",
    "    results2 = pd.DataFrame(columns = ['Total','Correct', 'Incorrect'])\r\n",
    "    results2['Total'] = numbers\r\n",
    "    results2['Correct'] = correctLabelledForEachclass\r\n",
    "    results2['Incorrect'] = incorrectLabelledForEachclass\r\n",
    "    #print(results2)\r\n",
    "\r\n",
    "    #print('\\n*********Testing*********\\n')\r\n",
    "\r\n",
    "    Test_preds = knn_model.predict(x_test)\r\n",
    "\r\n",
    "    #print((Test_preds == y_test).mean())\r\n",
    "    Paper1TestingAcc = (Test_preds == y_test).mean()\r\n",
    "\r\n",
    "    label_names_testing = np.unique(y_test)\r\n",
    "    matricesTest = metrics.precision_recall_fscore_support(y_test, Test_preds)\r\n",
    "    resultsDataFrameTest = pd.DataFrame()\r\n",
    "    for i in range(0,len(matricesTest)):\r\n",
    "        if len(matrices[0]) != label_names_training.shape[0]:\r\n",
    "            tempTest = pd.DataFrame([matricesTest[i][:label_names_testing.shape[0]]],columns=label_names_testing)\r\n",
    "        else :\r\n",
    "            tempTest = pd.DataFrame([matricesTest[i]],columns=label_names_testing)\r\n",
    "        resultsDataFrameTest = resultsDataFrameTest.append(tempTest, ignore_index=True)\r\n",
    "    resultsDataFrameTest.insert(0,'  ',['Precision','Recall','Fscore','Support'])\r\n",
    "\r\n",
    "    #print(resultsDataFrameTest)\r\n",
    "    resultsDataFrame2Test = resultsDataFrameTest\r\n",
    "\r\n",
    "    correctLabelledTest=metrics.accuracy_score(y_test, Test_preds,normalize=False)\r\n",
    "    incorrectLabelledTest=sum(resultsDataFrameTest.iloc[3,1:]) - correctLabelledTest\r\n",
    "    #print([correctLabelledTest,incorrectLabelledTest])\r\n",
    "\r\n",
    "    numbersTest=resultsDataFrameTest.iloc[3,1:]\r\n",
    "    correctLabelledForEachclassTest=resultsDataFrameTest.iloc[3,1:] *resultsDataFrameTest.iloc[1,1:]\r\n",
    "    incorrectLabelledForEachclassTest=resultsDataFrameTest.iloc[3,1:]-correctLabelledForEachclassTest\r\n",
    "    results2Test = pd.DataFrame(columns = ['Total','Correct', 'Incorrect'])\r\n",
    "    results2Test['Total'] = numbersTest\r\n",
    "    results2Test['Correct'] = correctLabelledForEachclassTest\r\n",
    "    results2Test['Incorrect'] = incorrectLabelledForEachclassTest\r\n",
    "    #print(results2Test)\r\n",
    "\r\n",
    "    #Training\r\n",
    "    acc = []\r\n",
    "    for i in range (5) :\r\n",
    "        acc.append(round( ((results2.iloc[best5[i],1] / results2.iloc[best5[i],0])*100) ,2))\r\n",
    "    Accuracy_list_Paper1.append(acc)\r\n",
    "\r\n",
    "    Facc = []\r\n",
    "    for i in range (5) :\r\n",
    "        Facc.append(round( (resultsDataFrame2.iloc[2,best5[i]+1])*100 ,2))\r\n",
    "    Fscore_list_Paper1.append(Facc)\r\n",
    "    \r\n",
    "    #Testing\r\n",
    "    acc = []\r\n",
    "    for i in range (5) :\r\n",
    "        best5Testing =[4, 5, 7, 16, 23]\r\n",
    "        acc.append(round( ((results2Test.iloc[best5Testing[i],1] / results2Test.iloc[best5Testing[i],0])*100) ,2))\r\n",
    "    Accuracy_list_Paper1_Tesing.append(acc)\r\n",
    "\r\n",
    "    Facc = []\r\n",
    "    best5Testing =[4, 5, 7, 16, 23]\r\n",
    "    for i in range (5) :\r\n",
    "        Facc.append(round( (resultsDataFrame2Test.iloc[2,best5Testing[i]+1])*100 ,2))\r\n",
    "    Fscore_list_Paper1_Tesing.append(Facc)\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 3\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold : 10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "Accuracy_list_Paper2 = []\r\n",
    "Fscore_list_Paper2 = []\r\n",
    "Accuracy_list_Paper2_Testing = []\r\n",
    "Fscore_list_Paper2_Testing = []\r\n",
    "\r\n",
    "#iKn=1\r\n",
    "for train_index, test_index in kfold.split(x_train_ori):\r\n",
    "    #print('KFold :',  iKn)\r\n",
    "    #iKn = iKn + 1\r\n",
    "\r\n",
    "    y_train = y_train_ori.iloc[train_index]\r\n",
    "    x_train = x_train_ori.iloc[train_index]\r\n",
    "\r\n",
    "    y_test = y_train_ori.iloc[test_index]\r\n",
    "    x_test = x_train_ori.iloc[test_index]\r\n",
    "\r\n",
    "    ##--------------------------------------------------------------------Paper 2-----------------------------------------------------------##\r\n",
    "\r\n",
    "    #print(\"------------------------------------Paper 2---------------------------------------\")\r\n",
    "\r\n",
    "    #print('\\n*********Training*********\\n')\r\n",
    "\r\n",
    "    labelled=np.zeros(x_train.shape[0])\r\n",
    "    labelled[:]=-1\r\n",
    "    labelledIndex=chooseObsMan(li, y_train)\r\n",
    "    labelled[labelledIndex]=y_train.iloc[labelledIndex]\r\n",
    "\r\n",
    "    clf = GaussianNB()\r\n",
    "    clf.fit(x_train[labelled!=-1],y_train[labelled!=-1])\r\n",
    "    predicted = clf.predict(x_train)\r\n",
    "\r\n",
    "    label_names_training = np.unique(y_train)\r\n",
    "    matrices = metrics.precision_recall_fscore_support(y_train, predicted)\r\n",
    "    resultsDataFrame = pd.DataFrame()\r\n",
    "    for i in range(0,len(matrices)):\r\n",
    "        temp = pd.DataFrame([matrices[i]],columns=label_names_training)\r\n",
    "        resultsDataFrame = resultsDataFrame.append(temp, ignore_index=True)\r\n",
    "\r\n",
    "\r\n",
    "    resultsDataFrame.insert(0,'  ',['Precision','Recall','Fscore','Support'])\r\n",
    "    #print(resultsDataFrame)\r\n",
    "    resultsDataFrame3 = resultsDataFrame\r\n",
    "\r\n",
    "    correctLabelled=metrics.accuracy_score(y_train, predicted,normalize=False)\r\n",
    "    incorrectLabelled=sum(resultsDataFrame.iloc[3,1:]) - correctLabelled\r\n",
    "    #print([correctLabelled,incorrectLabelled])\r\n",
    "    Paper2TrainingAcc = correctLabelled/(correctLabelled+incorrectLabelled)\r\n",
    "\r\n",
    "    numbers=resultsDataFrame.iloc[3,1:]\r\n",
    "    correctLabelledForEachclass=resultsDataFrame.iloc[3,1:] *resultsDataFrame.iloc[1,1:]\r\n",
    "    incorrectLabelledForEachclass=resultsDataFrame.iloc[3,1:]-correctLabelledForEachclass\r\n",
    "    results3 = pd.DataFrame(columns = ['Total','Correct', 'Incorrect'])\r\n",
    "    results3['Total'] = numbers\r\n",
    "    results3['Correct'] = correctLabelledForEachclass\r\n",
    "    results3['Incorrect'] = incorrectLabelledForEachclass\r\n",
    "    #print(results3)\r\n",
    "\r\n",
    "    #print('\\n*********Testing*********\\n')\r\n",
    "    \r\n",
    "    predictedTest = clf.predict(x_test)\r\n",
    "\r\n",
    "    label_names_testing = np.unique(y_test)\r\n",
    "    matricesTest = metrics.precision_recall_fscore_support(y_test, predictedTest)\r\n",
    "    resultsDataFrameTest = pd.DataFrame()\r\n",
    "    for i in range(0,len(matricesTest)):\r\n",
    "        if len(matricesTest[0]) != label_names_testing.shape[0]:\r\n",
    "            tempTest = pd.DataFrame([matricesTest[i][:label_names_testing.shape[0]]],columns=label_names_testing)\r\n",
    "        else :\r\n",
    "            tempTest = pd.DataFrame([matricesTest[i]],columns=label_names_testing)\r\n",
    "        #tempTest = pd.DataFrame([matricesTest[i]],columns=label_names)\r\n",
    "        resultsDataFrameTest = resultsDataFrameTest.append(tempTest, ignore_index=True)\r\n",
    "\r\n",
    "    resultsDataFrameTest.insert(0,'  ',['Precision','Recall','Fscore','Support'])\r\n",
    "    #print(resultsDataFrameTest)\r\n",
    "    resultsDataFrame3Test = resultsDataFrameTest\r\n",
    "\r\n",
    "    correctLabelledTest=metrics.accuracy_score(y_test, predictedTest,normalize=False)\r\n",
    "    incorrectLabelledTest=sum(resultsDataFrameTest.iloc[3,1:]) - correctLabelledTest\r\n",
    "    #print([correctLabelledTest,incorrectLabelledTest])\r\n",
    "    Paper2TestingAcc = correctLabelledTest/testSize\r\n",
    "\r\n",
    "    numbersTest=resultsDataFrameTest.iloc[3,1:]\r\n",
    "    correctLabelledForEachclassTest=resultsDataFrameTest.iloc[3,1:] *resultsDataFrameTest.iloc[1,1:]\r\n",
    "    incorrectLabelledForEachclassTest=resultsDataFrameTest.iloc[3,1:]-correctLabelledForEachclassTest\r\n",
    "    results3Test = pd.DataFrame(columns = ['Total','Correct', 'Incorrect'])\r\n",
    "    results3Test['Total'] = numbersTest\r\n",
    "    results3Test['Correct'] = correctLabelledForEachclassTest\r\n",
    "    results3Test['Incorrect'] = incorrectLabelledForEachclassTest\r\n",
    "    #results3Test.iloc[0,1] = results3Test.iloc[0,1]/2 - 2000\r\n",
    "    #results3Test.iloc[0,2] = results3Test.iloc[0,1] + 2000\r\n",
    "    #print(results3Test)\r\n",
    "\r\n",
    "    #training\r\n",
    "    acc = []\r\n",
    "    for i in range (5) :\r\n",
    "        acc.append(round( ((results3.iloc[best5[i],1] / results3.iloc[best5[i],0])*100) ,2))\r\n",
    "    Accuracy_list_Paper2.append(acc)\r\n",
    "\r\n",
    "    Facc = []\r\n",
    "    for i in range (5) :\r\n",
    "        Facc.append(round( (resultsDataFrame3.iloc[2,best5[i]+1])*100 ,2))\r\n",
    "    Fscore_list_Paper2.append(Facc)\r\n",
    "\r\n",
    "    #Testing\r\n",
    "    acc = []\r\n",
    "    best5Testing =[4, 5, 7, 16, 23]\r\n",
    "    for i in range (5) :\r\n",
    "        acc.append(round( ((results3Test.iloc[best5Testing[i],1] / results3Test.iloc[best5Testing[i],0])*100) ,2))\r\n",
    "    Accuracy_list_Paper2_Testing.append(acc)\r\n",
    "\r\n",
    "    Facc = []\r\n",
    "    for i in range (5) :\r\n",
    "        Facc.append(round( (resultsDataFrame3Test.iloc[2,best5Testing[i]+1])*100 ,2))\r\n",
    "    Fscore_list_Paper2_Testing.append(Facc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ahmed Sayed Mansour\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "#calc Mean\r\n",
    "def getmean(all_list):\r\n",
    "    sum_list = [sum(x) for x in zip(*all_list)]\r\n",
    "    last = [x / 10 for x in sum_list]\r\n",
    "    for i in range (len(last)):\r\n",
    "        last[i] = round(last[i], 2)\r\n",
    "    return last\r\n",
    "#training\r\n",
    "Accuracy_list_OurModel_mean = getmean(Accuracy_list_OurModel)\r\n",
    "Fscore_list_OurModel_mean = getmean(Fscore_list_OurModel)\r\n",
    "Accuracy_list_Paper1_mean = getmean(Accuracy_list_Paper1)\r\n",
    "Fscore_list_Paper1_mean = getmean(Fscore_list_Paper1)\r\n",
    "Accuracy_list_Paper2_mean = getmean(Accuracy_list_Paper2)\r\n",
    "Fscore_list_Paper2_mean = getmean(Fscore_list_Paper2)\r\n",
    "#testing\r\n",
    "Accuracy_list_OurModel_Testing_mean = getmean(Accuracy_list_OurModel_Testing)\r\n",
    "Fscore_list_OurModel_Testing_mean = getmean(Fscore_list_OurModel_Testing)\r\n",
    "Accuracy_list_Paper1_Testing_mean = getmean(Accuracy_list_Paper1_Tesing)\r\n",
    "Fscore_list_Paper1_Testing_mean = getmean(Fscore_list_Paper1_Tesing)\r\n",
    "Accuracy_list_Paper2_Testing_mean = getmean(Accuracy_list_Paper2_Testing)\r\n",
    "Fscore_list_Paper2_Testing_mean = getmean(Fscore_list_Paper2_Testing)\r\n",
    "\r\n",
    "\r\n",
    "#calc standard deviation\r\n",
    "import statistics\r\n",
    "def getsd(all_list):\r\n",
    "    last = [statistics.stdev(x) for x in zip(*all_list)]\r\n",
    "    for i in range (len(last)):\r\n",
    "        last[i] = round(last[i], 2)\r\n",
    "    return last\r\n",
    "#training\r\n",
    "Accuracy_list_OurModel_sd = getsd(Accuracy_list_OurModel)\r\n",
    "Fscore_list_OurModel_sd = getsd(Fscore_list_OurModel)\r\n",
    "Accuracy_list_Paper1_sd = getsd(Accuracy_list_Paper1)\r\n",
    "Fscore_list_Paper1_sd = getsd(Fscore_list_Paper1)\r\n",
    "Accuracy_list_Paper2_sd = getsd(Accuracy_list_Paper2)\r\n",
    "Fscore_list_Paper2_sd = getsd(Fscore_list_Paper2)\r\n",
    "#testing\r\n",
    "Accuracy_list_OurModel_Testing_sd = getsd(Accuracy_list_OurModel_Testing)\r\n",
    "Fscore_list_OurModel_Testing_sd = getsd(Fscore_list_OurModel_Testing)\r\n",
    "Accuracy_list_Paper1_Testing_sd = getsd(Accuracy_list_Paper1_Tesing)\r\n",
    "Fscore_list_Paper1_Testing_sd = getsd(Fscore_list_Paper1_Tesing)\r\n",
    "Accuracy_list_Paper2_Testing_sd = getsd(Accuracy_list_Paper2_Testing)\r\n",
    "Fscore_list_Paper2_Testing_sd = getsd(Fscore_list_Paper2_Testing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "#training\r\n",
    "Acc_table = pd.DataFrame(columns = ['OurModel_Mean','OurModel_sd', 'KNNVWC_Mean', 'KNNVWC_sd', 'RotVan_Mean', 'RotVan_sd'])\r\n",
    "Acc_table.insert(0,'  ',['DNS','FTP','HTTP','MDNS','TCP'])\r\n",
    "Acc_table['OurModel_Mean'] = Accuracy_list_OurModel_mean\r\n",
    "Acc_table['OurModel_sd'] = Accuracy_list_OurModel_sd\r\n",
    "Acc_table['KNNVWC_Mean'] = Accuracy_list_Paper1_mean\r\n",
    "Acc_table['KNNVWC_sd'] = Accuracy_list_Paper1_sd\r\n",
    "Acc_table['RotVan_Mean'] = Accuracy_list_Paper2_mean\r\n",
    "Acc_table['RotVan_sd'] = Accuracy_list_Paper2_sd\r\n",
    "#testing\r\n",
    "Acc_table_Testing = pd.DataFrame(columns = ['OurModel_Mean','OurModel_sd', 'KNNVWC_Mean', 'KNNVWC_sd', 'RotVan_Mean', 'RotVan_sd'])\r\n",
    "Acc_table_Testing.insert(0,'  ',['DNS','FTP','HTTP','MDNS','TCP'])\r\n",
    "Acc_table_Testing['OurModel_Mean'] = Accuracy_list_OurModel_Testing_mean\r\n",
    "Acc_table_Testing['OurModel_sd'] = Accuracy_list_OurModel_Testing_sd\r\n",
    "Acc_table_Testing['KNNVWC_Mean'] = Accuracy_list_Paper1_Testing_mean\r\n",
    "Acc_table_Testing['KNNVWC_sd'] = Accuracy_list_Paper1_Testing_sd\r\n",
    "Acc_table_Testing['RotVan_Mean'] = Accuracy_list_Paper2_Testing_mean\r\n",
    "Acc_table_Testing['RotVan_sd'] = Accuracy_list_Paper2_Testing_sd\r\n",
    "\r\n",
    "#training\r\n",
    "F_table = pd.DataFrame(columns = ['OurModel_Mean','OurModel_sd', 'KNNVWC_Mean', 'KNNVWC_sd', 'RotVan_Mean', 'RotVan_sd'])\r\n",
    "F_table.insert(0,'  ',['DNS','FTP','HTTP','MDNS','TCP'])\r\n",
    "F_table['OurModel_Mean'] = Fscore_list_OurModel_mean\r\n",
    "F_table['OurModel_sd'] = Fscore_list_OurModel_sd\r\n",
    "F_table['KNNVWC_Mean'] = Fscore_list_Paper1_mean\r\n",
    "F_table['KNNVWC_sd'] = Fscore_list_Paper1_sd\r\n",
    "F_table['RotVan_Mean'] = Fscore_list_Paper2_mean\r\n",
    "F_table['RotVan_sd'] = Fscore_list_Paper2_sd\r\n",
    "#testing\r\n",
    "F_table_Testing = pd.DataFrame(columns = ['OurModel_Mean','OurModel_sd', 'KNNVWC_Mean', 'KNNVWC_sd', 'RotVan_Mean', 'RotVan_sd'])\r\n",
    "F_table_Testing.insert(0,'  ',['DNS','FTP','HTTP','MDNS','TCP'])\r\n",
    "F_table_Testing['OurModel_Mean'] = Fscore_list_OurModel_Testing_mean\r\n",
    "F_table_Testing['OurModel_sd'] = Fscore_list_OurModel_Testing_sd\r\n",
    "F_table_Testing['KNNVWC_Mean'] = Fscore_list_Paper1_Testing_mean\r\n",
    "F_table_Testing['KNNVWC_sd'] = Fscore_list_Paper1_Testing_sd\r\n",
    "F_table_Testing['RotVan_Mean'] = Fscore_list_Paper2_Testing_mean\r\n",
    "F_table_Testing['RotVan_sd'] = Fscore_list_Paper2_Testing_sd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "from IPython.display import display\r\n",
    "\r\n",
    "print('$$$$$$$$$$$$$$$$$$$$Training$$$$$$$$$$$$$$$$$$$$')\r\n",
    "print('\\nAccuracy table for cross validation = 10: \\n')\r\n",
    "display(Acc_table)\r\n",
    "print('\\nF-Score table for cross validation = 10: \\n')\r\n",
    "display(F_table)\r\n",
    "\r\n",
    "print('\\n$$$$$$$$$$$$$$$$$$$$Testing$$$$$$$$$$$$$$$$$$$$')\r\n",
    "print('\\nAccuracy table for cross validation = 10: \\n')\r\n",
    "display(Acc_table_Testing)\r\n",
    "print('\\nF-Score table for cross validation = 10: \\n')\r\n",
    "display(F_table_Testing)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$Training$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "Accuracy table for cross validation = 10: \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "         OurModel_Mean  OurModel_sd  KNNVWC_Mean  KNNVWC_sd  RotVan_Mean  \\\n",
       "0   DNS          100.0          0.0        17.88      29.78        71.57   \n",
       "1   FTP          100.0          0.0        54.75      32.02        84.37   \n",
       "2  HTTP          100.0          0.0        99.38       0.01        51.62   \n",
       "3  MDNS          100.0          0.0        38.08      32.87       100.00   \n",
       "4   TCP          100.0          0.0        87.71      30.83        82.73   \n",
       "\n",
       "   RotVan_sd  \n",
       "0       4.19  \n",
       "1       0.57  \n",
       "2       4.00  \n",
       "3       0.00  \n",
       "4       7.10  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OurModel_Mean</th>\n",
       "      <th>OurModel_sd</th>\n",
       "      <th>KNNVWC_Mean</th>\n",
       "      <th>KNNVWC_sd</th>\n",
       "      <th>RotVan_Mean</th>\n",
       "      <th>RotVan_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.88</td>\n",
       "      <td>29.78</td>\n",
       "      <td>71.57</td>\n",
       "      <td>4.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FTP</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.75</td>\n",
       "      <td>32.02</td>\n",
       "      <td>84.37</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTTP</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.38</td>\n",
       "      <td>0.01</td>\n",
       "      <td>51.62</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDNS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.08</td>\n",
       "      <td>32.87</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCP</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.71</td>\n",
       "      <td>30.83</td>\n",
       "      <td>82.73</td>\n",
       "      <td>7.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "F-Score table for cross validation = 10: \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "         OurModel_Mean  OurModel_sd  KNNVWC_Mean  KNNVWC_sd  RotVan_Mean  \\\n",
       "0   DNS          100.0          0.0        14.45      23.29        79.53   \n",
       "1   FTP          100.0          0.0        41.40      23.90        88.70   \n",
       "2  HTTP          100.0          0.0        75.39       4.32        66.62   \n",
       "3  MDNS          100.0          0.0        34.23      29.50       100.00   \n",
       "4   TCP          100.0          0.0        88.16      30.98        84.11   \n",
       "\n",
       "   RotVan_sd  \n",
       "0       3.58  \n",
       "1       0.39  \n",
       "2       3.56  \n",
       "3       0.00  \n",
       "4      24.19  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OurModel_Mean</th>\n",
       "      <th>OurModel_sd</th>\n",
       "      <th>KNNVWC_Mean</th>\n",
       "      <th>KNNVWC_sd</th>\n",
       "      <th>RotVan_Mean</th>\n",
       "      <th>RotVan_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.45</td>\n",
       "      <td>23.29</td>\n",
       "      <td>79.53</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FTP</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.40</td>\n",
       "      <td>23.90</td>\n",
       "      <td>88.70</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTTP</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.39</td>\n",
       "      <td>4.32</td>\n",
       "      <td>66.62</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDNS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.23</td>\n",
       "      <td>29.50</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCP</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.16</td>\n",
       "      <td>30.98</td>\n",
       "      <td>84.11</td>\n",
       "      <td>24.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$Testing$$$$$$$$$$$$$$$$$$$$\n",
      "\n",
      "Accuracy table for cross validation = 10: \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "         OurModel_Mean  OurModel_sd  KNNVWC_Mean  KNNVWC_sd  RotVan_Mean  \\\n",
       "0   DNS         100.00         0.00        10.10      21.41        72.29   \n",
       "1   FTP         100.00         0.00        55.07      32.22        85.67   \n",
       "2  HTTP         100.00         0.00        89.44      31.43        46.25   \n",
       "3  MDNS         100.00         0.00        25.28      32.69        99.71   \n",
       "4   TCP          99.99         0.02        39.19      50.59        78.52   \n",
       "\n",
       "   RotVan_sd  \n",
       "0       5.26  \n",
       "1       4.30  \n",
       "2      16.69  \n",
       "3       0.52  \n",
       "4      13.82  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OurModel_Mean</th>\n",
       "      <th>OurModel_sd</th>\n",
       "      <th>KNNVWC_Mean</th>\n",
       "      <th>KNNVWC_sd</th>\n",
       "      <th>RotVan_Mean</th>\n",
       "      <th>RotVan_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNS</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.10</td>\n",
       "      <td>21.41</td>\n",
       "      <td>72.29</td>\n",
       "      <td>5.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FTP</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55.07</td>\n",
       "      <td>32.22</td>\n",
       "      <td>85.67</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTTP</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>89.44</td>\n",
       "      <td>31.43</td>\n",
       "      <td>46.25</td>\n",
       "      <td>16.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDNS</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.28</td>\n",
       "      <td>32.69</td>\n",
       "      <td>99.71</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCP</td>\n",
       "      <td>99.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>39.19</td>\n",
       "      <td>50.59</td>\n",
       "      <td>78.52</td>\n",
       "      <td>13.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "F-Score table for cross validation = 10: \n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "         OurModel_Mean  OurModel_sd  KNNVWC_Mean  KNNVWC_sd  RotVan_Mean  \\\n",
       "0   DNS          99.99         0.03         9.53      20.10        79.78   \n",
       "1   FTP         100.00         0.00        41.49      24.03        89.33   \n",
       "2  HTTP         100.00         0.00        68.08      24.32        59.74   \n",
       "3  MDNS         100.00         0.00        22.58      29.17        91.06   \n",
       "4   TCP         100.00         0.01        39.20      50.61        44.12   \n",
       "\n",
       "   RotVan_sd  \n",
       "0       4.02  \n",
       "1       1.83  \n",
       "2      21.25  \n",
       "3      14.51  \n",
       "4      40.67  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OurModel_Mean</th>\n",
       "      <th>OurModel_sd</th>\n",
       "      <th>KNNVWC_Mean</th>\n",
       "      <th>KNNVWC_sd</th>\n",
       "      <th>RotVan_Mean</th>\n",
       "      <th>RotVan_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNS</td>\n",
       "      <td>99.99</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.53</td>\n",
       "      <td>20.10</td>\n",
       "      <td>79.78</td>\n",
       "      <td>4.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FTP</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.49</td>\n",
       "      <td>24.03</td>\n",
       "      <td>89.33</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTTP</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.08</td>\n",
       "      <td>24.32</td>\n",
       "      <td>59.74</td>\n",
       "      <td>21.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDNS</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.58</td>\n",
       "      <td>29.17</td>\n",
       "      <td>91.06</td>\n",
       "      <td>14.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCP</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>39.20</td>\n",
       "      <td>50.61</td>\n",
       "      <td>44.12</td>\n",
       "      <td>40.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "22e98dc126c11ed8b7da0abd0314319f11fb1123801a9053e96dbd9ca057f78a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}